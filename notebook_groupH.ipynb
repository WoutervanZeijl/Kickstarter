{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np, operator\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 32, 16\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('KS_test_data.csv', delimiter=',')\n",
    "df_train = pd.read_csv('KS_train_data.csv', delimiter=',')\n",
    "df_train = df_train.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length of name feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length of name\n",
    "temp = []\n",
    "for i in df_train.name:\n",
    "    a = len(i)\n",
    "    b = np.log(a)\n",
    "    temp.append(b)\n",
    "se = pd.Series(temp)\n",
    "df_train['length_name'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length of name\n",
    "temp = []\n",
    "for i in df.name:\n",
    "    a = len(i)\n",
    "    b = np.log(a)\n",
    "    temp.append(b)\n",
    "se = pd.Series(temp)\n",
    "df['length_name'] = se.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Requirements feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#days between deadline and launched_at\n",
    "temp = []\n",
    "for i, j in zip(df_train.deadline, df_train.launched_at):\n",
    "    unix_timestamp = df_train.launched_at\n",
    "    utc_time = datetime.fromtimestamp(i)\n",
    "    unix_timestamp1 = df_train.deadline\n",
    "    utc_time1 = datetime.fromtimestamp(j)\n",
    "    temp.append((utc_time - utc_time1).days)\n",
    "se = pd.Series(temp)\n",
    "df_train['deadline_launched_at'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#days between deadline and launched_at\n",
    "temp = []\n",
    "for i, j in zip(df.deadline, df.launched_at):\n",
    "    unix_timestamp = df.launched_at\n",
    "    utc_time = datetime.fromtimestamp(i)\n",
    "    unix_timestamp1 = df.deadline\n",
    "    utc_time1 = datetime.fromtimestamp(j)\n",
    "    temp.append((utc_time - utc_time1).days)\n",
    "se = pd.Series(temp)\n",
    "df['deadline_launched_at'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_requirement = pd.DataFrame()\n",
    "df_daily_requirement['goal'] = df_train['goal']\n",
    "df_daily_requirement['deadline_launched_at'] = df_train['deadline_launched_at']\n",
    "df_daily_requirement['result'] = df_daily_requirement['goal']/df_daily_requirement['deadline_launched_at']\n",
    "df_train['log_daily_requirement'] = np.log(df_daily_requirement['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_requirement = pd.DataFrame()\n",
    "df_daily_requirement['goal'] = df['goal']\n",
    "df_daily_requirement['deadline_launched_at'] = df['deadline_launched_at']\n",
    "df_daily_requirement['result'] = df_daily_requirement['goal']/df_daily_requirement['deadline_launched_at']\n",
    "df['log_daily_requirement'] = np.log(df_daily_requirement['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['project_id', 'backers_count', 'blurb', 'category', 'country',\n",
      "       'created_at', 'currency', 'deadline', 'fx_rate', 'goal', 'launched_at',\n",
      "       'name', 'staff_pick', 'location', 'subcategory', 'project_url',\n",
      "       'reward_url', 'length_name', 'deadline_launched_at',\n",
      "       'log_daily_requirement'],\n",
      "      dtype='object')\n",
      "Index(['project_id', 'backers_count', 'blurb', 'category',\n",
      "       'converted_pledged_amount', 'country', 'created_at', 'currency',\n",
      "       'deadline', 'fx_rate', 'goal', 'launched_at', 'name', 'pledged',\n",
      "       'staff_pick', 'usd_pledged', 'location', 'funded', 'subcategory',\n",
      "       'project_url', 'reward_url', 'length_name', 'deadline_launched_at',\n",
      "       'log_daily_requirement'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funded on words feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\woute\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "words_funded = {}\n",
    "\n",
    "for i in df_train[df_train['funded'] == True]['blurb']:\n",
    "    words = i.split()\n",
    "    for word in words:\n",
    "        if word.lower() in cachedStopWords:\n",
    "            pass\n",
    "        else:\n",
    "            if word in words_funded:\n",
    "                words_funded[word] += 1\n",
    "            else:\n",
    "                words_funded[word] = 1\n",
    "                \n",
    "words_not_funded = {}\n",
    "\n",
    "for i in df_train[df_train['funded'] == False]['blurb']:\n",
    "    words = i.split()\n",
    "    for word in words:\n",
    "        try:\n",
    "            if word.lower() in cachedStopWords:\n",
    "                pass\n",
    "            else:\n",
    "                if word in words_not_funded:\n",
    "                    words_not_funded[word] += 1\n",
    "                else:\n",
    "                    words_not_funded[word] = 1\n",
    "        except:\n",
    "            pass\n",
    "#for key, value in sorted(words_funded.iteritems(), key=lambda (k,v): (v,k), reverse=True):\n",
    "#    print \"%s: %s\" % (key, value)\n",
    "#for key, value in sorted(words_funded.iteritems(), key=lambda (k,v): (v,k), reverse=True):\n",
    "#    print \"%s: %s\" % (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_funded = {k: v for k, v in words_funded.items() if v > 10}\n",
    "ratio_funded_unfunded = (float(len(df_train[df_train['funded'] == True]['funded']))) / (float(len(df_train[df_train['funded'] == False]['funded'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ratio = []\n",
    "for i in df['blurb']:\n",
    "    try:\n",
    "        i = i.split()\n",
    "    except:\n",
    "        #assume it will fail if blurb is null (in a sense)\n",
    "        list_ratio.append(0.01)\n",
    "        continue\n",
    "    count = 0\n",
    "    ratio_number = 0 \n",
    "    for word in i:\n",
    "        if word in words_funded and word in words_not_funded:\n",
    "                count += 1\n",
    "                ratio_number += (float(words_funded[word]) /float(ratio_funded_unfunded)) / float(words_not_funded[word])\n",
    "        else:\n",
    "            count += 1\n",
    "            ratio_number += 1\n",
    "    ratio_number = ratio_number / count\n",
    "    list_ratio.append(ratio_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ratio = []\n",
    "for i in df_train['blurb']:\n",
    "    try:\n",
    "        i = i.split()\n",
    "    except:\n",
    "        #assume it will fail if blurb is null (in a sense)\n",
    "        list_ratio.append(0.01)\n",
    "        continue\n",
    "    count = 0\n",
    "    ratio_number = 0 \n",
    "    for word in i:\n",
    "        if word in words_funded and word in words_not_funded:\n",
    "                count += 1\n",
    "                ratio_number += (float(words_funded[word]) /float(ratio_funded_unfunded)) / float(words_not_funded[word])\n",
    "        else:\n",
    "            count += 1\n",
    "            ratio_number += 1\n",
    "    ratio_number = ratio_number / count\n",
    "    list_ratio.append(ratio_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['funded_on_words'] = list_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ratio = []\n",
    "for i in df['blurb']:\n",
    "    try:\n",
    "        i = i.split()\n",
    "    except:\n",
    "        #assume it will fail if blurb is null (in a sense)\n",
    "        list_ratio.append(0.01)\n",
    "        continue\n",
    "    count = 0\n",
    "    ratio_number = 0 \n",
    "    for word in i:\n",
    "        if word in words_funded and word in words_not_funded:\n",
    "                count += 1\n",
    "                ratio_number += (float(words_funded[word]) /float(ratio_funded_unfunded)) / float(words_not_funded[word])\n",
    "        else:\n",
    "            count += 1\n",
    "            ratio_number += 1\n",
    "    ratio_number = ratio_number / count\n",
    "    list_ratio.append(ratio_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['funded_on_words'] = list_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['project_id', 'backers_count', 'blurb', 'category',\n",
      "       'converted_pledged_amount', 'country', 'created_at', 'currency',\n",
      "       'deadline', 'fx_rate', 'goal', 'launched_at', 'name', 'pledged',\n",
      "       'staff_pick', 'usd_pledged', 'location', 'funded', 'subcategory',\n",
      "       'project_url', 'reward_url', 'length_name', 'deadline_launched_at',\n",
      "       'log_daily_requirement', 'funded_on_words'],\n",
      "      dtype='object')\n",
      "Index(['project_id', 'backers_count', 'blurb', 'category', 'country',\n",
      "       'created_at', 'currency', 'deadline', 'fx_rate', 'goal', 'launched_at',\n",
      "       'name', 'staff_pick', 'location', 'subcategory', 'project_url',\n",
      "       'reward_url', 'length_name', 'deadline_launched_at',\n",
      "       'log_daily_requirement', 'funded_on_words'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_train.columns)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['staff_pick', 'funded_on_words', 'length_name', 'log_daily_requirement']\n",
    "\n",
    "X = df_train[features]\n",
    "y = df_train['funded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [10000 10001 10002 ... 99992 99993 99994] TEST: [   0    1    2 ... 9997 9998 9999]\n",
      "TRAIN: [    0     1     2 ... 99992 99993 99994] TEST: [10000 10001 10002 ... 19997 19998 19999]\n",
      "TRAIN: [    0     1     2 ... 99992 99993 99994] TEST: [20000 20001 20002 ... 29997 29998 29999]\n",
      "TRAIN: [    0     1     2 ... 99992 99993 99994] TEST: [30000 30001 30002 ... 39997 39998 39999]\n",
      "TRAIN: [    0     1     2 ... 99992 99993 99994] TEST: [40000 40001 40002 ... 49997 49998 49999]\n",
      "TRAIN: [    0     1     2 ... 99992 99993 99994] TEST: [50000 50001 50002 ... 59996 59997 59998]\n",
      "TRAIN: [    0     1     2 ... 99992 99993 99994] TEST: [59999 60000 60001 ... 69995 69996 69997]\n",
      "TRAIN: [    0     1     2 ... 99992 99993 99994] TEST: [69998 69999 70000 ... 79994 79995 79996]\n",
      "TRAIN: [    0     1     2 ... 99992 99993 99994] TEST: [79997 79998 79999 ... 89993 89994 89995]\n",
      "TRAIN: [    0     1     2 ... 89993 89994 89995] TEST: [89996 89997 89998 ... 99992 99993 99994]\n",
      "model 0 scores: train 0.7484415800877826\ttest 0.7178\n",
      "model 1 scores: train 0.7419967775987555\ttest 0.7669\n",
      "model 2 scores: train 0.7477859881104506\ttest 0.7107\n",
      "model 3 scores: train 0.7478082115673093\ttest 0.7182\n",
      "model 4 scores: train 0.7463636868714929\ttest 0.732\n",
      "model 5 scores: train 0.7469109738210588\ttest 0.727972797279728\n",
      "model 6 scores: train 0.7417996355393573\ttest 0.7773777377737774\n",
      "model 7 scores: train 0.7418107471443175\ttest 0.7716771677167716\n",
      "model 8 scores: train 0.7452997911018268\ttest 0.7280728072807281\n",
      "model 9 scores: train 0.7414440641806302\ttest 0.7841784178417842\n",
      "avg_train 0.7449661456022981\n",
      "avg_test 0.7434878927892788\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "sum_train_score = 0\n",
    "sum_test_score = 0\n",
    "model_list = []\n",
    "score_list_train = []\n",
    "score_list_test = []\n",
    "\n",
    "\n",
    "KFold(n_splits=2, random_state=None, shuffle=False)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    sum_train_score += model.score(X_train, y_train)\n",
    "    sum_test_score += model.score(X_test, y_test)\n",
    "    #print(model.score(X_train, y_train))\n",
    "    #print(model.score(X_test, y_test))\n",
    "    prediction_train = model.predict(X_train)\n",
    "    prediction_test = model.predict(X_test)\n",
    "    #print(model.predict(X_train))\n",
    "    #print(model.predict(X_test))\n",
    "    model_list.append(model)\n",
    "    score_list_train.append(model.score(X_train, y_train))\n",
    "    score_list_test.append(model.score(X_test, y_test))\n",
    "    \n",
    "    \n",
    "for i,model in enumerate(model_list):\n",
    "    print(\"model {} scores: train {}\\ttest {}\".format(i, score_list_train[i], score_list_test[i]))\n",
    "    \n",
    "print(\"avg_train\", sum_train_score/10)\n",
    "print(\"avg_test\", sum_test_score/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79996, 4) (79996,)\n",
      "(19999, 4) (19999,)\n",
      "0.7458997949897495\n",
      "0.7430871543577179\n",
      "[ True False  True ... False  True False]\n",
      "[False  True False ...  True  True  True]\n",
      "confusion matrix train\n",
      " [[22564  9799]\n",
      " [10528 37105]]\n",
      "confusion matrix  test\n",
      " [[5739 2426]\n",
      " [2712 9122]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn import metrics\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "prediction_train = model.predict(X_train)\n",
    "print(prediction_train)\n",
    "prediction_test = model.predict(X_test)\n",
    "print(prediction_test)\n",
    "\n",
    "confusion_matrix_train = metrics.confusion_matrix(prediction_train, y_train)\n",
    "confusion_matrix_test = metrics.confusion_matrix(prediction_test, y_test)\n",
    "\n",
    "print(\"confusion matrix train\\n\", confusion_matrix_train)\n",
    "print(\"confusion matrix  test\\n\", confusion_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_predictions = model.predict(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = pd.Series(list_predictions)\n",
    "df['funded_prediction'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('predictions.tsv', sep='\\t', columns=['project_id','funded_prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "TN_train = confusion_matrix_train[0][0]\n",
    "FN_train = confusion_matrix_train[1][0]\n",
    "TP_train = confusion_matrix_train[1][1]\n",
    "FP_train = confusion_matrix_train[0][1]\n",
    "\n",
    "total_train = confusion_matrix_train.sum()\n",
    "actual_train_yes = TP_train + FN_train\n",
    "actual_train_no = TN_train + FP_train\n",
    "predicted_train_yes = FP_train + TP_train\n",
    "\n",
    "#test\n",
    "TN_test = confusion_matrix_test[0][0]\n",
    "FN_test = confusion_matrix_test[1][0]\n",
    "TP_test = confusion_matrix_test[1][1]\n",
    "FP_test = confusion_matrix_test[0][1]\n",
    "\n",
    "total_test = confusion_matrix_test.sum()\n",
    "actual_test_yes = TP_test + FN_test\n",
    "actual_test_no = TN_test + FP_test\n",
    "predicted_test_yes = FP_test + TP_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: Overall, how often is the classifier correct? 0.7458997949897495\n",
      "Misclassification Rate: Overall, how often is it wrong? 0.2541002050102505\n",
      "True Positive Rate: When it's actually yes, how often does it predict yes? 0.7789767598093759\n",
      "False Positive Rate: When it's actually no, how often does it predict yes? 0.3027840435064734\n",
      "Specificity: When it's actually no, how often does it predict no? 0.6972159564935265\n",
      "Precision: When it predicts yes, how often is it correct? 0.791083916083916\n",
      "Prevalence: How often does the yes condition actually occur in our sample? 0.5954422721136057\n",
      "\n",
      "\n",
      "Accuracy: Overall, how often is the classifier correct? 0.7430871543577179\n",
      "Misclassification Rate: Overall, how often is it wrong? 0.2569128456422821\n",
      "True Positive Rate: When it's actually yes, how often does it predict yes? 0.770829812404935\n",
      "False Positive Rate: When it's actually no, how often does it predict yes? 0.29712186160440907\n",
      "Specificity: When it's actually no, how often does it predict no? 0.702878138395591\n",
      "Precision: When it predicts yes, how often is it correct? 0.7899203325251126\n",
      "Prevalence: How often does the yes condition actually occur in our sample? 0.5917295864793239\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "print(\"Accuracy: Overall, how often is the classifier correct?\", ((TP_train+TN_train)/total_train))\n",
    "print(\"Misclassification Rate: Overall, how often is it wrong?\", ((FP_train+FN_train)/total_train))\n",
    "print(\"True Positive Rate: When it's actually yes, how often does it predict yes?\", (TP_train/actual_train_yes))\n",
    "print(\"False Positive Rate: When it's actually no, how often does it predict yes?\", (FP_train/actual_train_no))\n",
    "print(\"Specificity: When it's actually no, how often does it predict no?\", (TN_train/actual_train_no))\n",
    "print(\"Precision: When it predicts yes, how often is it correct?\", (TP_train/predicted_train_yes))\n",
    "print(\"Prevalence: How often does the yes condition actually occur in our sample?\", actual_train_yes/total_train)\n",
    "print(\"\\n\")\n",
    "#test\n",
    "print(\"Accuracy: Overall, how often is the classifier correct?\", ((TP_test+TN_test)/total_test))\n",
    "print(\"Misclassification Rate: Overall, how often is it wrong?\", ((FP_test+FN_test)/total_test))\n",
    "print(\"True Positive Rate: When it's actually yes, how often does it predict yes?\", (TP_test/actual_test_yes))\n",
    "print(\"False Positive Rate: When it's actually no, how often does it predict yes?\", (FP_test/actual_test_no))\n",
    "print(\"Specificity: When it's actually no, how often does it predict no?\", (TN_test/actual_test_no))\n",
    "specificity = TN_test/actual_test_no\n",
    "print(\"Precision: When it predicts yes, how often is it correct?\", (TP_test/predicted_test_yes))\n",
    "precision = (TP_test/predicted_test_yes)\n",
    "print(\"Prevalence: How often does the yes condition actually occur in our sample?\", actual_test_yes/total_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46859\n",
      "31206\n",
      "Specificity: When it's actually no, how often does it predict no?  21934.015186772813 / 31206\n",
      "Precision: When it predicts yes, how often is it correct?  37014.876861794255 / 46859\n",
      "estimate:  58949\n"
     ]
    }
   ],
   "source": [
    "total_true_predictions = df[df['funded_prediction'] == True]['funded_prediction'].count()\n",
    "print(total_true_predictions)\n",
    "total_false_predictions = df[df['funded_prediction'] == False]['funded_prediction'].count()\n",
    "print(total_false_predictions)\n",
    "\n",
    "specificity_estimate = (specificity * total_false_predictions)\n",
    "print(\"Specificity: When it's actually no, how often does it predict no? \", specificity_estimate, \"/\", total_false_predictions)\n",
    "precision_estimate = (precision * total_true_predictions)\n",
    "print(\"Precision: When it predicts yes, how often is it correct? \", precision_estimate, \"/\", total_true_predictions )\n",
    "\n",
    "\n",
    "estimate = specificity_estimate + precision_estimate\n",
    "print(\"estimate: \", int(round(estimate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting funded feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_predictions = model.predict(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = pd.Series(list_predictions)\n",
    "df['funded_prediction'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     78065\n",
       "unique        2\n",
       "top        True\n",
       "freq      46859\n",
       "Name: funded_prediction, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.funded_prediction.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_funded_unfunded_comparison = (float(len(df[df['funded_prediction'] == True]['funded_prediction']))) / (float(len(df[df['funded_prediction'] == False]['funded_prediction'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5016022559764148"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_funded_unfunded_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN WITH EVOLUTIONARY COMPUTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_near_algorithm(one_population):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=one_population[0], algorithm=one_population[2], metric=one_population[3], weights=one_population[1])\n",
    "    neigh.fit(X_train, y_train)\n",
    "    score = neigh.score(X_test, y_test)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankParameters(population):\n",
    "    global fitnessResults\n",
    "    fitnessResults = {}\n",
    "\n",
    "    for i in range(len(population)):\n",
    "        fitnessResults[i] = k_near_algorithm(population[i])\n",
    "    return sorted(fitnessResults.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection(popRanked, eliteSize):\n",
    "    selectionResults = []\n",
    "    df = pd.DataFrame(np.array(popRanked), columns=[\"Index\", \"Fitness\"])\n",
    "    df['cum_sum'] = df.Fitness.cumsum()\n",
    "    df['cum_perc'] = 100 * df.cum_sum / df.Fitness.sum()\n",
    "\n",
    "    for i in range(0, eliteSize):\n",
    "        selectionResults.append(popRanked[i][0])\n",
    "    for i in range(0, len(popRanked) - eliteSize):\n",
    "        pick = 100 * random.random()\n",
    "        for i in range(0, len(popRanked)):\n",
    "            if pick <= df.iat[i, 3]:\n",
    "                selectionResults.append(popRanked[i][0])\n",
    "                break\n",
    "    return selectionResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matingPool(population, selectionResults):\n",
    "    matingpool = []\n",
    "    for i in range(0, len(selectionResults)):\n",
    "        index = selectionResults[i]\n",
    "        matingpool.append(population[index])\n",
    "    return matingpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breed(parent1, parent2):\n",
    "    child = []\n",
    "    \n",
    "    \n",
    "    first_gene = [parent1[0], parent2[0]]\n",
    "    second_gene = [parent1[1], parent2[1]]\n",
    "    third_gene = [parent1[2], parent2[2]]\n",
    "    fourth_gene = [parent1[3], parent2[3]]\n",
    "\n",
    "    child.append(random.sample(first_gene, 1)[0])\n",
    "    child.append(random.sample(second_gene, 1)[0])\n",
    "    child.append(random.sample(third_gene, 1)[0])\n",
    "    child.append(random.sample(fourth_gene, 1)[0])\n",
    "    \n",
    "    \n",
    "    return child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breedPopulation(matingpool, eliteSize):\n",
    "    children = []\n",
    "    length = len(matingpool) - eliteSize\n",
    "    pool = random.sample(matingpool, len(matingpool))\n",
    "\n",
    "    for i in range(0, eliteSize):\n",
    "        children.append(matingpool[i])\n",
    "\n",
    "    for i in range(0, length):\n",
    "        child = breed(pool[i], pool[len(matingpool) - i - 1])\n",
    "        children.append(child)\n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(individual, mutationRate, population):\n",
    "    mutated_child = []\n",
    "    for i in range(len(population)):\n",
    "        if (random.random() < mutationRate):\n",
    "            element_changed = int(random.random() * len(individual))\n",
    "            random_sample = random.sample(population, 1)[0]\n",
    "\n",
    "            if element_changed == 0:\n",
    "                mutated_child.append(int(random.random() * 100 + 1))\n",
    "                mutated_child.append(individual[1])\n",
    "                mutated_child.append(individual[2])\n",
    "                mutated_child.append(individual[3])\n",
    "            elif element_changed == 1:\n",
    "                mutated_child.append(individual[0])\n",
    "                mutated_child.append(random_sample[element_changed])\n",
    "                mutated_child.append(individual[2])\n",
    "                mutated_child.append(individual[3])\n",
    "            elif element_changed == 2:\n",
    "                mutated_child.append(individual[0])\n",
    "                mutated_child.append(individual[1])\n",
    "                mutated_child.append(random_sample[element_changed])\n",
    "                mutated_child.append(individual[3])\n",
    "            else:\n",
    "                mutated_child.append(individual[0])\n",
    "                mutated_child.append(individual[1])\n",
    "                mutated_child.append(individual[2])\n",
    "                mutated_child.append(random_sample[element_changed])\n",
    "        break\n",
    "        \n",
    "    if not mutated_child:\n",
    "        #print('individual ', individual)\n",
    "        return individual\n",
    "    else:\n",
    "        #print('mutated_child ', mutated_child)\n",
    "        return mutated_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutatePopulation(population, mutationRate):\n",
    "    mutatedPop = []\n",
    "    \n",
    "    for i in range(0, len(population)):\n",
    "        mutated = mutate(population[i], mutationRate, population)\n",
    "        mutatedPop.append(mutated)\n",
    "    return mutatedPop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextGeneration(currentGen, eliteSize, mutationRate):\n",
    "    popRanked = rankParameters(currentGen) #popRanked is list of 25 tuples with sorted best fit with each fit corresponding index value\n",
    "    selectionResults = selection(popRanked, eliteSize) # A list of sorted index values of best fit, based on preference\n",
    "    matingpool = matingPool(currentGen, selectionResults) #currentGen is the original data --> parameter_list,                                   \n",
    "                                                          #so mating pool is ordered list of parameters with best fit value\n",
    "    children = breedPopulation(matingpool, eliteSize)\n",
    "    nextGeneration = mutatePopulation(children, mutationRate)\n",
    "    \n",
    "    return nextGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithm(population, eliteSize, mutationRate, generations):\n",
    "    pop = population\n",
    "    print ('Initial score: ', str(rankParameters(population)[0][1]))\n",
    "    for i in range(0, generations):\n",
    "        pop = nextGeneration(pop, eliteSize, mutationRate)\n",
    "\n",
    "    print(\"Final score: \" + str(rankParameters(pop)[0][1]))\n",
    "    bestParameterIndex = rankParameters(pop)[0][0]\n",
    "    bestParameter = pop[bestParameterIndex]\n",
    "    print(bestParameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneticAlgorithmPlot(population, eliteSize, mutationRate, generations):\n",
    "    pop = population\n",
    "    progress = []\n",
    "    progress.append((rankParameters(population)[0][1]))\n",
    "\n",
    "    for i in range(0, generations):\n",
    "        pop = nextGeneration(pop, eliteSize, mutationRate)\n",
    "        progress.append((rankParameters(population)[0][1]))\n",
    "        print (i)\n",
    "\n",
    "    plt.plot(progress)\n",
    "    plt.ylabel('fitness')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE POPULATION\n",
    "populationSize = 20\n",
    "import random\n",
    "\n",
    "n_neighbours = range(3, 100)\n",
    "weights = ['uniform']\n",
    "algorithm = ['ball_tree', 'kd_tree', 'auto']\n",
    "metric = ['euclidean', 'manhattan', 'chebyshev']\n",
    "\n",
    "parameter_list = []\n",
    "\n",
    "while ((len(parameter_list) < populationSize)):\n",
    "    for i in range(populationSize):\n",
    "        new_list = []\n",
    "        new_list.append(random.sample(n_neighbours, 1)[0])\n",
    "        new_list.append(random.sample(weights, 1)[0])\n",
    "        new_list.append(random.sample(algorithm, 1)[0])\n",
    "        new_list.append(random.sample(metric, 1)[0])\n",
    "        neigh = KNeighborsClassifier(n_neighbors=new_list[0], algorithm=new_list[2], metric=new_list[3], weights=new_list[1])\n",
    "        neigh.fit(X_train, y_train)\n",
    "        try:\n",
    "            if neigh.score(X_test, y_test) < 0.95:\n",
    "                parameter_list.append(new_list)\n",
    "        except:\n",
    "            print('Not enough memory')\n",
    "        \n",
    "parameter_list = parameter_list[:populationSize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running evolutionary computing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial score:  0.744037201860093\n",
      "Final score: 0.744037201860093\n",
      "[86, 'uniform', 'kd_tree', 'manhattan']\n"
     ]
    }
   ],
   "source": [
    "geneticAlgorithm(population=parameter_list, eliteSize=2, mutationRate=0.1, generations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running KNN model with evolutionary computing parameters outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.7550752537626881\n",
      "test  score 0.7489374468723436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "neigh = KNeighborsClassifier(n_neighbors=38, algorithm=\"auto\", metric=\"manhattan\", weights=\"uniform\")\n",
    "neigh.fit(X_train, y_train)\n",
    "print(\"train score\", neigh.score(X_train, y_train))\n",
    "print(\"test  score\", neigh.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions knn + evolutionary computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_predictions = neigh.predict(df[features])\n",
    "se = pd.Series(list_predictions)\n",
    "df['funded_prediction_knn'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('predictions_knn+evolutionary_computing.tsv', sep='\\t', columns=['project_id','funded_prediction_knn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matric KNN + evolutionary computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy train 0.7550752537626881\n",
      "accuracy  test 0.7489374468723436\n",
      "confusion matrix train\n",
      " [[23163  9464]\n",
      " [10129 37240]]\n",
      "confusion matrix  test\n",
      " [[5655 2425]\n",
      " [2596 9323]]\n"
     ]
    }
   ],
   "source": [
    "prediction_train = neigh.predict(X_train)\n",
    "prediction_test = neigh.predict(X_test)\n",
    "print(\"accuracy train\", metrics.accuracy_score(prediction_train, y_train))\n",
    "print(\"accuracy  test\", metrics.accuracy_score(prediction_test, y_test))\n",
    "confusion_matrix_train_knn = metrics.confusion_matrix(prediction_train, y_train)\n",
    "confusion_matrix_test_knn = metrics.confusion_matrix(prediction_test, y_test)\n",
    "print(\"confusion matrix train\\n\", confusion_matrix_train_knn)\n",
    "print(\"confusion matrix  test\\n\", confusion_matrix_test_knn)\n",
    "\n",
    "#train\n",
    "TN_train_knn = confusion_matrix_train_knn[0][0]\n",
    "FN_train_knn = confusion_matrix_train_knn[1][0]\n",
    "TP_train_knn = confusion_matrix_train_knn[1][1]\n",
    "FP_train_knn = confusion_matrix_train_knn[0][1]\n",
    "\n",
    "total_train_knn = confusion_matrix_train.sum()\n",
    "actual_train_yes_knn = TP_train_knn + FN_train_knn\n",
    "actual_train_no_knn = TN_train_knn + FP_train_knn\n",
    "predicted_train_yes_knn = FP_train_knn + TP_train_knn\n",
    "\n",
    "#test\n",
    "TN_test_knn = confusion_matrix_test_knn[0][0]\n",
    "FN_test_knn = confusion_matrix_test_knn[1][0]\n",
    "TP_test_knn = confusion_matrix_test_knn[1][1]\n",
    "FP_test_knn = confusion_matrix_test_knn[0][1]\n",
    "\n",
    "total_test_knn = confusion_matrix_test.sum()\n",
    "actual_test_yes_knn = TP_test_knn + FN_test_knn\n",
    "actual_test_no_knn = TN_test_knn + FP_test_knn\n",
    "predicted_test_yes_knn = FP_test_knn + TP_test_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: Overall, how often is the classifier correct? 0.7550752537626881\n",
      "Misclassification Rate: Overall, how often is it wrong? 0.24492474623731186\n",
      "True Positive Rate: When it's actually yes, how often does it predict yes? 0.7861681690557115\n",
      "False Positive Rate: When it's actually no, how often does it predict yes? 0.29006650933276124\n",
      "Specificity: When it's actually no, how often does it predict no? 0.7099334906672388\n",
      "Precision: When it predicts yes, how often is it correct? 0.7973621103117506\n",
      "Prevalence: How often does the yes condition actually occur in our sample? 0.5921421071053553\n",
      "\n",
      "\n",
      "Accuracy: Overall, how often is the classifier correct? 0.7489374468723436\n",
      "Misclassification Rate: Overall, how often is it wrong? 0.2510625531276564\n",
      "True Positive Rate: When it's actually yes, how often does it predict yes? 0.7821964929943788\n",
      "False Positive Rate: When it's actually no, how often does it predict yes? 0.3001237623762376\n",
      "Specificity: When it's actually no, how often does it predict no? 0.6998762376237624\n",
      "Precision: When it predicts yes, how often is it correct? 0.7935818862785154\n",
      "Prevalence: How often does the yes condition actually occur in our sample? 0.5959797989899495\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "print(\"Accuracy: Overall, how often is the classifier correct?\", ((TP_train_knn+TN_train_knn)/total_train_knn))\n",
    "print(\"Misclassification Rate: Overall, how often is it wrong?\", ((FP_train_knn+FN_train_knn)/total_train_knn))\n",
    "print(\"True Positive Rate: When it's actually yes, how often does it predict yes?\", (TP_train_knn/actual_train_yes_knn))\n",
    "print(\"False Positive Rate: When it's actually no, how often does it predict yes?\", (FP_train_knn/actual_train_no_knn))\n",
    "print(\"Specificity: When it's actually no, how often does it predict no?\", (TN_train_knn/actual_train_no_knn))\n",
    "print(\"Precision: When it predicts yes, how often is it correct?\", (TP_train_knn/predicted_train_yes_knn))\n",
    "print(\"Prevalence: How often does the yes condition actually occur in our sample?\", actual_train_yes_knn/total_train_knn)\n",
    "print(\"\\n\")\n",
    "#test\n",
    "print(\"Accuracy: Overall, how often is the classifier correct?\", ((TP_test_knn+TN_test_knn)/total_test_knn))\n",
    "print(\"Misclassification Rate: Overall, how often is it wrong?\", ((FP_test_knn+FN_test_knn)/total_test_knn))\n",
    "print(\"True Positive Rate: When it's actually yes, how often does it predict yes?\", (TP_test_knn/actual_test_yes_knn))\n",
    "print(\"False Positive Rate: When it's actually no, how often does it predict yes?\", (FP_test_knn/actual_test_no_knn))\n",
    "print(\"Specificity: When it's actually no, how often does it predict no?\", (TN_test_knn/actual_test_no_knn))\n",
    "specificity = TN_test_knn/actual_test_no_knn\n",
    "print(\"Precision: When it predicts yes, how often is it correct?\", (TP_test_knn/predicted_test_yes_knn))\n",
    "precision = (TP_test/predicted_test_yes)\n",
    "print(\"Prevalence: How often does the yes condition actually occur in our sample?\", actual_test_yes_knn/total_test_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation KNN + evolutionary computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46859\n",
      "31206\n",
      "Specificity: When it's actually no, how often does it predict no?  21840.33787128713 / 31206\n",
      "Precision: When it predicts yes, how often is it correct?  37014.876861794255 / 46859\n",
      "estimate:  58855\n"
     ]
    }
   ],
   "source": [
    "total_true_predictions = df[df['funded_prediction'] == True]['funded_prediction'].count()\n",
    "print(total_true_predictions)\n",
    "total_false_predictions = df[df['funded_prediction'] == False]['funded_prediction'].count()\n",
    "print(total_false_predictions)\n",
    "\n",
    "specificity_estimate = (specificity * total_false_predictions)\n",
    "print(\"Specificity: When it's actually no, how often does it predict no? \", specificity_estimate, \"/\", total_false_predictions)\n",
    "precision_estimate = (precision * total_true_predictions)\n",
    "print(\"Precision: When it predicts yes, how often is it correct? \", precision_estimate, \"/\", total_true_predictions )\n",
    "\n",
    "estimate = specificity_estimate + precision_estimate\n",
    "print(\"estimate: \", int(round(estimate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False False]\n",
      "[2 1 3 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "svm = LogisticRegression()\n",
    "# create the RFE model for the svm classifier \n",
    "# and select attributes\n",
    "rfe = RFE(svm, 1)\n",
    "rfe = rfe.fit(X_test, y_test)\n",
    "# print summaries for the selection of attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
